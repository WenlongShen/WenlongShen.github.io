---
layout:     post
title:      "Deep Learning Book 学习笔记（10）"
subtitle:   "Sequence Modeling: Recurrent and Recursive Nets"
date:       2018-05-16
author:     "Wenlong Shen"
header-img: "img/bg/2018_3.jpg"
tags: 机器学习 读书笔记 2018
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

*循环递归，无限往复*

#### 循环神经网络

循环神经网络是专门用于处理序列（包括可变长度的序列）的神经网络，其设计模式包括以下几种：1、每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络；2、每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络；3. 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络。

#### 双向RNN

循环神经网络还可以允许t时刻的状态依赖于整个输入序列，即可以从“过去”和“未来”中获得信息，并称之为双向循环神经网络，下图是一个典型的双向RNN：
![rnn](/img/post/2018_05_16_rnn1.png)

#### 基于编码-解码的序列到序列架构

RNN的输入还可以是可变长度的，想法很简单，即，1、由编码器或读取器或输入RNN处理输入序列，编码器输出上下文C；2、解码器或写入器或输出RNN则以固定长度的向量为条件产生输出序列Y。

#### 深度循环网络

大多数RNN中的计算可以分解成三块参数及其相关的变换：1、从输入到隐藏状态；2、从前一隐藏状态到下一隐藏状态；3、从隐藏状态到输出。这三个块都与单个权重矩阵相关联，而我们可以将深度MLP应用其中以增加深度。

#### 递归神经网络

递归神经网络是循环网络的另一个扩展，它被构造为深的树状结构而不是RNN的链状结构，一个例图如下：
![rnn](/img/post/2018_05_16_rnn2.png)
递归网络的深度通过非线性操作可以急剧减小，从而有助于解决长期依赖的问题，不过相应的难点在于如何选择树的结构。

#### 长期依赖的挑战

循环网络涉及相同函数的多次组合，每个时间步一次，这些组合可以导致极端非线性行为。长期依赖的根本问题是经过许多阶段传播后的梯度倾向于消失或爆炸，即使我们假设循环网络是参数稳定的，也会存在来自比短期相互作用指数小的权重所带来的困难。

具体来说，每当模型能够表示长期依赖时，长期相互作用的梯度幅值就会变得指数小，这并不意味着这是不可能学习的，由于长期依赖关系的信号很容易被短期相关性产生的最小波动隐藏，因而学习长期依赖可能需要很长的时间。当我们增加了需要捕获的依赖关系的跨度，基于梯度的优化变得越来越困难。

#### 回声状态网络

循环权重映射是循环网络中最难学习的参数，为避免这种困难，我们可以设定循环隐藏单元，使其能很好地捕捉过去输入历史，并且只学习输出权重。基于这种思想，提出了回声状态网络，将任意长度的序列映射为一个长度固定的向量，之后可以施加一个线性预测算子以解决感兴趣的问题。训练准则就可以很容易地设计为输出权重的凸函数。

#### 渗漏单元和其他多时间尺度的策略

处理长期依赖的一种方法是设计工作在多个时间尺度的模型，使模型的某些部分在细粒度时间尺度上操作并能处理小细节，而其他部分在粗时间尺度上操作并能把遥远过去的信息更有效地传递过来。存在多种同时构建粗细时间尺度的策略。这些策略包括在时间轴增加跳跃连接，“渗漏单元”使用不同时间常数整合信号，并去除一些用于建模细粒度时间尺度的连接。

#### 长短期记忆和其他门控RNN

像渗漏单元一样，门控RNN想法也是基于生成通过时间的路径，其中导数既不消失也不发生爆炸。渗漏单元通过手动选择常量的连接权重或参数化的连接权重来达到这一目的。门控RNN将其推广为在每个时间步都可能改变的连接权重。渗漏单元允许网络在较长持续时间内积累信息（诸如用于特定特征或类的线索）。然而，一旦该信息被使用，让神经网络遗忘旧的状态可能是有用的。例如，如果一个序列是由子序列组成，我们希望渗漏单元能在各子序列内积累线索，我们需要将状态设置为0以忘记旧状态的机制。我们希望神经网络学会决定何时清除状态，而不是手动决定。这就是门控RNN 要做的事。
