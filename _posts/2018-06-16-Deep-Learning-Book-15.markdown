---
layout:     post
title:      "Deep Learning Book 学习笔记（15）"
subtitle:   "Representation Learning"
date:       2018-06-16
author:     "Wenlong Shen"
header-img: "img/bg/2018_3.jpg"
tags: 机器学习 读书笔记 2018
---

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=default"></script>

*大脑究竟是如何从非常少的标注样本中学习的？*

#### 贪心逐层无监督预训练

无监督学习在深度神经网络的复兴上起到了关键的、历史性的作用，它使研究者首次可以训练不含诸如卷积或者循环这类特殊结构的深度监督网络。我们将这一过程称为无监督预训练或者贪心逐层无监督预训练。此过程是一个任务（无监督学习，尝试获取输入分布的形状）的表示如何有助于另一个任务（具有相同输入域的监督学习）的典型示例。

贪心逐层无监督预训练依赖于单层表示学习算法，每一层使用无监督学习预训练，将前一层的输出作为输入，输出数据的新的表示。贪心逐层无监督预训练被称为贪心（greedy）的，是因为它是一个贪心算法，这意味着它独立地优化解决方案的每一个部分，每一步解决一个部分，而不是联合优化所有部分。它被称为逐层（layer-wise）的，是因为这些独立的解决方案是网络层。具体地，贪心逐层无监督预训练每次处理一层网络，训练第k层时保持前面的网络层不变。特别地，低层网络（最先训练的）不会在引入高层网络后进行调整。它被称为无监督（unsupervised）的，是因为每一层用无监督表示学习算法训练。然而，它也被称为预训练（pretraining），是因为它只是在联合训练算法精调所有层之前的第一步。在监督学习任务中，它可以被看作是正则化项（在一些实验中，预训练不能降低训练误差，但能降低测试误差）和参数初始化的一种形式。

#### 迁移学习和领域自适应

迁移学习和领域自适应指的是利用一个情景（例如，分布$$P_1$$）中已经学到的内容去改善另一个情景（比如分布$$P_2$$）中的泛化情况。在迁移学习（transfer learning）中，学习器必须执行两个或更多个不同的任务，但是我们假设能够解释$$P_1$$变化的许多因素和学习$$P_2$$需要抓住的变化相关。这通常能够在监督学习中解释，输入是相同的，但是输出不同的性质。在领域自适应（domain adaption）的相关情况下，在每个情景之间任务（和最优的输入到输出的映射）都是相同的，但是输入分布稍有不同。

#### 半监督解释因果关系

表示学习的一个重要问题是“什么原因能够使一个表示比另一个表示更好？”一种假设是，理想表示中的特征对应到观测数据的潜在成因，特征空间中不同的特征或方向对应着不同的原因，从而表示能够区分这些原因。在表示学习的其他方法中，我们大多关注易于建模的表示，例如，数据稀疏或是各项之间相互独立的情况。能够清楚地分离出潜在因素的表示可能并不一定易于建模。然而，该假设促使半监督学习使用无监督表示学习的一个更深层原因是，对于很多人工智能任务而言，有两个相随的特点：一旦我们能够获得观察结果基本成因的解释，那么将会很容易分离出个体属性。

#### 分布式表示

分布式表示的概念（由很多元素组合的表示，这些元素之间可以设置成可分离的）是表示学习最重要的工具之一。具有多个隐藏单元的神经网络和具有多个潜变量的概率模型都利用了分布式表示的策略。

#### 得益于深度的指数增益

多层感知机是万能近似器，相比于浅层网络，一些函数能够用指数级小的深度网络表示，缩小模型规模能够提高统计效率。在许多不同情景中已经证明，非线性和重用特征层次结构的组合来组织计算，可以使分布式表示获得指数级加速之外，还可以获得统计效率的指数级提升。

#### 提供发现潜在原因的线索

正则化策略对于获得良好泛化是很有必要的。当不可能找到一个普遍良好的正则化策略时，深度学习的一个目标是找到一套相当通用的正则化策略，使其能够适用于各种各样的AI 任务。下述列表提供了一些通用正则化策略。

* 平滑：假设对于单位$$d$$和小量$$\epsilon$$有$$f(x+\epsilon d)\approx f(x)$$。这个假设允许学习器从训练样本泛化到输入空间中附近的点。许多机器学习算法都利用了这个想法，但它不能克服维数灾难难题。
* 线性：很多学习算法假定一些变量之间的关系是线性的。这使得算法能够预测远离观测数据的点，但有时可能会导致一些极端的预测。大多数简单的学习算法不会做平滑假设，而会做线性假设。
* 多个解释因子：许多表示学习算法受以下假设的启发，数据是由多个潜在解释因子生成的，并且给定每一个因子的状态，大多数任务都能轻易解决。
* 因果因子：该模型认为学成表示所描述的变差因素是观察数据$$x$$的成因，而并非反过来。这对于半监督学习是有利的，当潜在成因上的分布发生改变，或者我们应用模型到一个新的任务上时，学成的模型都会更加鲁棒。
* 深度，或者解释因子的层次组织：高级抽象概念能够通过将简单概念层次化来定义。从另一个角度来看，深度架构表达了我们认为任务应该由多个程序步骤完成的观念，其中每一个步骤回溯到先前步骤处理之后的输出。
* 任务间共享因素：当多个对应到不同变量$$y_i$$的任务共享相同的输入$$x$$时，或者当每个任务关联到全局输入$$x$$的子集或者函数$$f^i(x)$$时，我们会假设每个变量$$y_i$$关联到来自相关因素$$h$$公共池的不同子集。因为这些子集有重叠，所以通过共享的中间表示$$P(h\vert x)$$来学习所有的$$P(y_i\vert x)$$能够使任务间共享统计强度。
* 流形：概率质量集中，并且集中区域是局部连通的，且占据很小的体积。在连续情况下，这些区域可以用比数据所在原始空间低很多维的低维流形来近似。
* 自然聚类：很多机器学习算法假设输入空间中每个连通流形可以被分配一个单独的类。数据分布在许多个不连通的流形上，但相同流形上数据的类别是相同的。这个假设激励了各种学习算法，包括正切传播、双反向传播、流形正切分类器和对抗训练。
* 时间和空间相干性：慢特征分析和相关的算法假设，最重要的解释因子随时间变化很缓慢，或者至少假设预测真实的潜在解释因子比预测诸如像素值这类原始观察会更容易些。
* 稀疏性：假设大部分特征和大部分输入不相关，如在表示猫的图像时，没有必要使用象鼻的特征。因此，我们可以强加一个先验，任何可以解释为“存在”或“不存在”的特征在大多数时间都是不存在的。
* 简化因子依赖：在良好的高级表示中，因子会通过简单的依赖相互关联。最简单的可能是边缘独立，但是线性依赖或浅层自编码器所能表示的依赖关系也是合理的假设。这可以从许多物理定律中看出来，并且假设在学成表示的顶层插入线性预测器或分解的先验。
