---
layout:     post
title:      "RNA大语言模型简介"
subtitle:   "A brief introduction to RNA LLMs"
date:       2024-12-19
author:     "Wenlong Shen"
header-img: "img/bg/2024_1.jpg"
tags: RNA 大语言模型 2024
---

*天地有大美而不言，四时有明法而不议，万物有成理而不说。*

“Attention Is All You Need”。注意力机制、大语言模型的出现，将人工智能提升到了前所未有的高度，就此，人类社会的发展也进入了一个全新的维度。

而这一新的范式，究竟能为生物学领域的研究带来怎样的改变？答案似乎取决于这样一个基本的科学问题：生物学所有的秘密是否都藏于碱基/氨基酸序列之中？如果大语言模型框架足够复杂、数据量足够多，是否能在某个节点之后达到对生物学机制的“理解”？

终极的问题等待着天才们去探索，而现阶段的大语言模型已在生物学研究的各个领域不断突破，这里，我们把目光聚焦于RNA————作为DNA信息的传递者、蛋白质信息的翻译官，集序列和结构特征于一身的两面角色，看看大语言模型究竟能为研究者带来些什么。

#### 常见模型

我大致翻阅了近几年间常见的一些RNA大语言模型，简单列一下：RNABERT、RNA-FM、RNA-MSM、UNI-RNA、BiRNA-BERT、DGRNA、ERNIE-RNA、RNAErnie、GenerRNA、LAMAR、ProtRNA、RiNALMo、RNA-km等，大都以BERT为基础，在数据集的选取、具体的模型架构、参数的规模方面有所不同，另外的区别在于预训练任务和掩码策略。

预训练任务方面，常见的即MLM，也有将序列比对或者RNA分子类别作为预训练任务的。关键点就在于如何在自监督学习的框架下，更多更全面地解析序列中隐藏的生物学信息。目前的都是基于自然语言模型，所以一定存在偏性，因为自然语言重在信息的传递，而其本身不在物理层面发挥作用。但是考虑到RNA分子本身具有的结构特性，我们就要提出问题：仅依靠序列信息是否足够？即大模型是否能够从序列中学到一切，是否需要辅助有结构的信息（二级、三级结构所表示的碱基相互作用、旋转角等），这就限制了数据量（想要获取真实的结构需要进行大量实验，且RNA结构本身就是动态的，受多种其它分子、环境因素的影响），所以我们需要进一步思考序列和结构之间的关系，以及有何种方式能将二者统一？进一步带来的问题是，如果序列本身的信息不足以直接指导结构生成，我们就需要更多的理化知识来强化算法，但这些理化性质本身是否可以通过大语言模型习得？且如何习得？

掩码策略方面，可以直接使用单碱基作为token，或使用k-mer（k取3～8），也有模型利用motif数据库，以特定的motif作为token的，目的都在于让模型学习到碱基之间的“信息”关系。k-mer和motif都是为了带来RNA分子的属性、碱基偏性、结合位点等先验知识，如果在数据量和参数量足够的前提下，单碱基token体现了高分辨率和高信息量，是否能够学习到足够的知识？另外也有将MSA信息作为输入的，因为“保守性”在生物学研究中有着重要的地位。从这个角度出发，如何在不使用MSA的情况下，将“保守性”作为特征体现在模型学习的过程中？只依靠加大数据量是否足够（保守就是数据重复，喂给模型足够的“重复”数据，是否就能学习到“保守”这种特征）？

#### 下游任务

整合进下游任务时，一种简单的方式即直接使用模型对RNA序列提取embedding作为其特征矩阵；另外就是针对下游任务数据进行微调，即通过较少次数的训练迭代来更新参数，调整预训练模型的权重，这样可以学习下游任务的特征，并且提高模型性能。

针对RNA分子的下游任务，常见的如结构预测与设计、稳定性预测、mRNA翻译速率、分子结合位点预测等。对于结构预测，通常的方法是将大语言模型应用于序列，从而获得不同的embedding或者是attention map，进一步拼接成矩阵后输入ResNet或者Unet等进行预测。

#### 新的模型？

蛋白质大语言模型起步较早，其很多新的模型结构都值得RNA领域借鉴，如整合结构信息，构建图注意力模型等。DNA大语言模型的长处则是超长片段的处理，以及不同功能片段的分类预测等。

取长补短，综合考虑生物学功能和计算机算法，RNA大语言模型领域大有可为。

