---
layout:     post
title:      "Deep Learning Book 学习笔记（2）"
subtitle:   "Linear Algebra"
date:       2018-04-11
author:     "Wenlong Shen"
header-img: "img/bg/2018_3.jpg"
tags: 机器学习 读书笔记 2018
---

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=default"></script>

*线性代数是基础啊，多年未看，重新翻翻书吧。*

#### 标量、向量、矩阵和张量

**标量（scalar）**即为单独的数。**向量（vector）**是一列数，有序排列，能够索引，如向量$$x$$中的第一个元素$$x_1$$，需要明确表示时，可用如下形式：

$$
 x=\begin{bmatrix}
   x_1 \\
   x_2 \\
   \cdots \\
   x_n \\
  \end{bmatrix}
$$

**矩阵（matrix）**是一个二维数组，其中每一个元素拥有行、列两个索引，明确表示时，有如下形式：

$$
A=\left[
\begin{matrix}
 A_{1,1} & A_{1,2} & \cdots & A_{1,n} \\
 A_{2,1} & A_{2,2} & \cdots & A_{2,n} \\
 \vdots & \vdots & \ddots & \vdots \\
 A_{m,1} & A_{m,2} & \cdots & A_{m,n} \\
\end{matrix}
\right]
$$

**张量（tensor）**一般是指大于二维的数组（一维张量是向量，二维张量是矩阵）。需要注意转置（transpose）是矩阵和向量的常用操作，$$(A^T)_{i,j}=A_{j,i}$$

#### 矩阵和向量相乘

乘法是矩阵运算最重要的操作之一，**矩阵乘积（matrix  product）**要求矩阵A的列数和矩阵B的行数相等，即A为$$m\times n$$，B为$$n\times p$$，则乘积C为$$m\times p$$，具体的定义为：

$$C_{i,j}=\sum_kA_{i,k}B_{k,j}$$

两个维数相同的向量x和y的**点积（dot product）**可以看作矩阵乘积$$x^Ty$$，因而，矩阵乘积$$C=AB$$中计算$$C_{i,j}$$可以看作是A的第i行和B的第j列的点积。另外，矩阵乘法服从分配律和结合律，但是不满足交换律（点积满足交换律）。用矩阵、向量乘积更多地是为了紧凑地表示线性方程式：$$Ax=b$$

#### 单位矩阵和逆矩阵

为描述矩阵逆（matrix inversion），我们先定义**单位矩阵（identity matrix）**，单位矩阵的主对角线元素都是1，其余位置都是0，如

$$
I_3=\left[
\begin{matrix}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & 0 & 1 \\
\end{matrix}
\right]
$$

任何向量和单位矩阵相乘都保持不变。这时，我们定义**矩阵逆**，记作$$A^{-1}$$，满足条件：$$A^{-1}A=I$$，这样，就可以求解线性方程式：

$$
\begin{align}
Ax &= b \\
x &= A^{-1}b
\end{align}
$$

当然，上述求解的过程取决于是否存在逆矩阵$$A^{-1}$$，当逆矩阵存在时，有多种不同的算法都能找到它的闭解形式。理论上，相同的逆矩阵可用于多次求解不同向量$$b$$的方程。然而，逆矩阵$$A^{-1}$$主要是作为理论工具使用，并不会在大多数程序中实际使用。这是因为逆矩阵$$A^{-1}$$在计算机上只能表现出有限的精度，而那些有效使用向量$$b$$的算法通常可以得到更精确的$$x$$。

#### 线性相关和生成子空间

如果逆矩阵$$A^{-1}$$存在，那么对于每一个向量$$b$$恰好存在一个解。对于方程组而言，特定的向量$$b$$，要么不存在解，要么只有一个解，要么有无穷多的解，因为如果$$x$$和$$y$$都是解，则$$z=\alpha x+(1-\alpha)y$$也是解。为分析方程解的个数，我们可以将$$A$$的列向量看作从原点（origin）出发的不同方向，确定有多少种方法可以到达向量b，而向量$$x$$中的每个元素表示需要沿这些方向走多远：

$$
\begin{align}
b_i &= \sum_kA_{i,k}x_k \\
b &= \sum_kA_{:,k}x_k
\end{align}
$$

这种操作被称为**线性组合（linear combination）**。形式上，一组向量的线性组合即指每个向量乘以对应标量系数之后的和，即：$$\sum_i c_i v^{(i)}$$，相应地，一组向量的**生成子空间（span）**是其线性组合后所能抵达的点的集合。所以，$$Ax=b$$是否有解，即相当于向量$$b$$是否在$$A$$的列向量的生成子空间中，又称为$$A$$的列空间（column space）或值域（range）。

有了空间的概念，我们再讨论方程解的问题，比如为使$$Ax=b$$对于任意向量$$b$$在m维实数域中存在解，即要求$$A$$的列空间构成整个m维实数域，这就要求$$A$$至少有m列，这是有解的必要条件。实际的列向量之间可能存在**线性相关（linear dependence）**的情况，即某个向量可能是其它某些向量的线性组合，因而$$Ax=b$$有解的充要条件实际是要求$$A$$包含至少一组m个线性无关的列向量。

而想要使$$A$$可逆，还需要保证每一个$$b$$至多只有一个解，这就要求$$A$$至多只有m个线性无关的列向量。所以矩阵$$A$$必须是一个方阵（square），且所有列向量都是线性无关的。一个列向量线性相关的方阵被称为**奇异（singular）**。需要注意的是，如果矩阵$$A$$不是一个方阵或者是一个奇异的方阵，该方程仍然可能有解，但是无法使用矩阵逆的方法去求解。

#### 范数

有时候我们需要衡量向量的大小，可用到**范数（norm）**，它可以是满足下列性质的任意函数：

* $$f(x)=0 \Rightarrow x=0$$
* $$f(x+y) \leq f(x) + f(y)$$
* $$\forall \alpha \in R，f(\alpha x)=|\alpha|f(x)$$

直观上来说，范数将向量映射为一个非负值，衡量了其与原点之间的距离。机器学习中常用到**$$L^p$$范数**，其定义如下：

$$\|x\|_p = (\sum_i|x_i|^p)^{\frac{1}{p}}$$

其中$$p \geq 1$$。当p=2，即$$L^2$$范数又被称为欧几里得范数（Euclidean norm），反映了由向量$$x$$确定的点到原点的欧几里得距离。$$L^2$$范数的平方也常被使用，可以简单通过点积$$x^Tx$$来计算。不同情况下往往需要考虑使用不同范数，比如$$L^2$$范数的平方对$$x$$中每个元素的导数只取决于对应的元素，而$$L^2$$范数对每个元素的导数却和整个向量相关。但是$$L^2$$范数的平方在原点附近的增长十分缓慢，在一些需要区分零和非零（较小的）值的机器学习中，就不如$$L^1$$范数好用。另一个机器学习中常见的范数是$$L^ \infty$$，称为**最大范数（max norm）**，表示向量中值最大的元素的绝对值：$$\|x\|_\infty = \max \vert x_i \vert$$。类似于向量范数，也可以定义矩阵的范数，如**Frobenius范数**：$$\|A\|_F = \sqrt{\sum_{i,j}A_{i,j}^2}$$。

另外，两个向量的**点积（dot product）**也可以用范数表示，

$$x^Ty = \|x\|_2\|y\|_2\cos \theta$$

#### 特殊类型的矩阵和向量

再记录一些常见的矩阵和向量。**对角矩阵（diagonal matrix）**是指只在主对角线上含有非零元素，其他位置都是零的矩阵，即当且仅当$$i \ne j，D_{i,j}=0$$，比如单位矩阵。对角矩阵使得一些运算变得简单快捷，比如对角方阵逆矩阵存在的充要条件是其对角元素都是非零值，此时，$$diag(v)^{-1}=diag([1/v_i,...,1/v_n]^T)$$，在机器学习中通过将矩阵限制为对角矩阵，可以显著降低计算成本。**对称矩阵（symmetric matrix）**是指转置跟自己相等的矩阵，即$$A=A^T$$。**单位向量（unit vector）**是具有单位$$L^2$$范数的向量，即$$\sqrt{\sum_ix_i^2}=1$$。如果$$x^Ty=0$$，则$$x$$和$$y$$称为互相**正交（orthogonal）**，此时如果两个向量均非零，则其夹角为90度。在n维向量空间中，至多有n个范数非零的向量互相正交，若其范数均为1，则称它们是**标准正交（orthonormal）**。**正交矩阵（orthogonal matrix）**是指行向量和列向量标准正交的方阵，即$$A^TA=AA^T=I$$，此时亦有$$A^{-1}=A^T$$。

#### 特征分解

数学上常将复杂对象进行属性分解而进行更好地理解，矩阵亦如此。通过**特征分解（eigendecomposition）**将矩阵分解成一组**特征向量（eigenvector）**和**特征值（eigenvalue）**，定义式如下：

$$Av=\lambda v$$

其中标量$$\lambda$$称为特征向量$$v$$（非零向量）的特征值。可见，任意的$$\alpha v (\alpha \ne 0)$$也是$$A$$的特征向量，且它们具有相同的特征值，基于此，通常我们只考虑单位特征向量。

假设矩阵$$A$$有n个线性无关的特征向量$${v^{(1)},...,v^{(n)}}$$，对应的特征值为$${\lambda_1,...,\lambda_n}$$。我们将特征向量组成矩阵，每一列是一个特征向量，即$$V=[v^{(1)},...,v^{(n)}]$$，类似地，将特征值组成一个向量$$\lambda=[\lambda_1,...,\lambda_n]^T$$，此时，矩阵A的**特征分解（eigendecomposition）**可记作：

$$A=Vdiag(\lambda)V^{-1}$$

特征分解使得我们可以从多维空间的角度更好地理解矩阵特性（特征向量勾画空间，特征值表示拉伸倍数），然而并不是所有的矩阵都能够进行特征分解，有些还会涉及复数域，不过，每个实对称矩阵都可以分解为实特征向量和实特征值，即$$A=Qdiag(\Lambda)Q^T$$，其中$$Q$$是$$A$$的特征向量组成的正交矩阵，$$\Lambda$$是对角矩阵。需注意的是，实对称矩阵的特征分解可能不唯一，如果两个或多个特征向量拥有相同的特征值，那么在由这些特征向量产生的生成子空间中，任意一组正交向量都是该特征值对应的特征向量，因此，我们可以等价地从这些特征向量中构成$$Q$$作为替代。

特征分解唯一当且仅当所有的特征值都是唯一的，而矩阵是奇异的当且仅当含有零特征值。另外，所有特征值都是正数的矩阵被称为**正定（positive definite）**；所有特征值都是非负数的矩阵被称为半正定（positive semidefinite）。同样地，所有特征值都是负数的矩阵被称为负定（negative definite）；所有特征值都是非正数的矩阵被称为半负定（negative semidefinite）。

#### 奇异值分解

另一种矩阵分解方法是**奇异值分解（singular value decomposition，SVD）**，即将矩阵分解为**奇异向量（singular vector）**和**奇异值（singular value）**，有意思的是，实数矩阵不一定有特征分解，但一定有奇异值分解，因而其使用范围更广一些。奇异值分解记作：

$$A=UDV^T$$

假设$$A$$为$$m \times n$$，则$$U$$为$$m \times m$$，$$D$$为$$m \times n$$，$$V$$为$$n \times n$$，同时我们给这些矩阵赋予特殊结构，要求$$U$$和$$V$$为正交矩阵，$$D$$为对角矩阵（不一定是方阵）。

用特征分解去解释奇异值分解可知，$$A$$的**左奇异向量（left singular vector）**即$$U$$的列向量是$$AA^T$$的特征向量，$$A$$的**右奇异向量（left singular vector）**即$$V$$的列向量是$$A^TA$$的特征向量，$$A$$的非零奇异值既是$$AA^T$$特征值的平方根，也是$$A^TA$$特征值的平方根。

#### 其它

对于非方矩阵而言，没有逆矩阵的定义，求解$$Ax=y$$可以利用**Moore-Penrose伪逆**等方法。需注意，如果矩阵$$A$$的行数大于列数，那么上述方程可能没有解；如果矩阵$$A$$的行数小于列数，那么可能有多个解。**迹运算**是指矩阵对角元素之和。**行列式**记作$$det(A)$$，等于矩阵特征值的乘积。

#### 实例：主成分分析

**主成分分析（principal components analysis，PCA）**是一个最基本的机器学习算法，其目的在于降维，在损失精度与消耗算力（CPU、memory等）之间寻找性价比。比如对于每个$$x^{(i)}\in R^n$$，我们希望对应一个编码向量$$c^{(i)}\in R^l$$，让$$l<n$$，即达到了降维的目的，有编码函数$$f(x)=c$$，再有解码函数$$x\approx g(f(x))$$。

在PCA算法中，我们定义函数$$g(c)=Dc$$，其中$$D\in R^{n\times l}$$，然后使用$$L^2$$范数构建一个最优编码$$c^*=argmin\|x-g(c)\|_2$$，这里也往往用$$L^2$$范数的平方来代替，可推导出$$c=D^Tx$$。这里编码矩阵$$D$$的选择就至关重要了，可考虑最小化误差矩阵的Frobenius范数。最终的优化问题可以通过特征分解来求解，一般地，对于主成分的基，矩阵$$D$$由$$X$$的前$$l$$个最大特征值对应的特征向量组成。

