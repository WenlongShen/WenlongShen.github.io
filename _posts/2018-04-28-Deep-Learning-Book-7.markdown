---
layout:     post
title:      "Deep Learning Book 学习笔记（7）"
subtitle:   "Regularization for Deep Learning"
date:       2018-04-28
author:     "Wenlong Shen"
header-img: "img/bg/2018_3.jpg"
tags: 机器学习 读书笔记 2018
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

*泛化好才是真的好~*

机器学习的目的不是训练数据表现好，而是要泛化好，许多策略都显式地设计来减少误差，这些策略被统称为正则化。在实际的深度学习应用中总是会发现，最好的拟合模型是一个适当正则化的大型模型。

#### 参数范数惩罚

一些正则化方法对目标函数$$J$$添加一个参数范数惩罚$$\Omega(\theta)$$，以限制模型（如神经网络、线性回归、逻辑回归等）的学习能力。正则化后的目标函数记为$$\hat J$$：

$$\hat J(\theta; X,y)=J(\theta; X,y)+\alpha\Omega(\theta)$$

其中$$\alpha \in [0,\infty)$$是权衡范数惩罚项$$\Omega$$和标准目标函数$$J$$相对贡献的超参数，$$\alpha$$越大，对应的正则化惩罚越大。

令正则项$$\Omega(\theta)=\frac{1}{2}\|\omega\|^2_2$$即称为$$L^2$$参数范数惩罚。$$L^2$$正则化能让学习算法“感知”到具有较高方差的输入$$x$$，因此与输出目标的协方差较小（相对增加方差）的特征的权重将会收缩。令正则项$$\Omega(\theta)=\|\omega\|_1$$即为$$L^1$$正则化，为各个参数的绝对值之和。相比$$L^2$$正则化，$$L^1$$正则化会产生更稀疏的解，使部分子集的权重为零，表明相应的特征可以被安全地忽略。

#### 作为约束的范数惩罚

我们可以把参数范数惩罚看作对权重强加的约束，如果$$\Omega$$是$$L^2$$范数，那么权重就是被约束在一个$$L^2$$球中，如果$$\Omega$$是$$L^1$$范数，那么权重就是被约束在一个$$L^1$$范数限制的区域中。

有时候我们希望使用显式的限制，而不是惩罚，而且惩罚可能会导致目标函数非凸而使算法陷入局部极小，这时可通过修改下降算法，进行重投影等方法。一个推荐的策略是：约束神经网络层的权重矩阵每列的范数，而不是限制整个权重矩阵的Frobenius范数，其中分别限制每一列的范数可以防止某一隐藏单元有非常大的权重，而列范数的限制可以通过重投影的显式约束来实现。

#### 正则化和欠约束问题

在机器学习的许多模型中，包括线性回归和PCA，都依赖于对矩阵$$X^TX$$求逆。只要$$X^TX$$是奇异的，这些方法就会失效。在这种情况下，正则化的许多形式对应求逆$$X^TX+\alpha I$$，这个正则化矩阵可以保证是可逆的，也就能够保证应用于欠定问题的迭代方法收敛。

#### 数据集增强

让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练。但在实际中，我们拥有的数据量是有限的，解决这个问题的一种方法是创建假数据并添加到训练集中。另外，在神经网络的输入层注入噪声也可以被看作是数据增强的一种方式。

#### 噪声鲁棒性

将噪声作用于输入可以作为数据集增强的策略之一。对于某些模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚。在一般情况下，注入噪声远比简单地收缩参数强大，特别是噪声被添加到隐藏单元时会更加强大。另一种正则化模型的噪声使用方式是将其加到权重，这项技术主要用于循环神经网络。这可以被解释为关于权重的贝叶斯推断的随机实现，贝叶斯学习过程将权重视为不确定的，并且可以通过概率分布表示这种不确定性，向权重添加噪声是反映这种不确定性的一种实用的随机方法。

噪声还可以注入输出目标。由于大多数数据集的$$y$$标签都有一定错误，错误的$$y$$不利于最大化$$log p(y \vert x)$$，避免这种情况的一种方法是显式地对标签上的噪声进行建模。

#### 半监督学习

半监督学习，是将$$P(x)$$产生的未标记样本和$$P(x,y)$$中的标记样本都用于估计$$P(y \vert x)$$。可以构建这样一个模型，其中生成模型$$P(x)$$或$$P(x,y)$$与判别模型$$P(y \vert x)$$共享参数，而不用分离无监督和监督部分。

#### 多任务学习

多任务学习是指将几个任务中的样例合并（可以视为对参数施加的软约束）来提高泛化能力的一种方式。如同额外的训练样本能够提高泛化能力一样，多个额外的任务共享通常也会带来更好的泛化能力。

#### 提前终止

在一些大数据量的过拟合模型中，我们经常能看到训练误差随着时间的推移逐渐降低，而验证集的误差再次上升的情况。这意味着训练模型并不是越多步数越长时间就越好，我们需要存储记录训练过程中使验证集误差最低的参数设置，即当验证集上的误差在事先指定的循环次数内没有进一步改善时，算法就会终止。这种策略被称为提前终止，由于其简单有效而在深度学习的正则化方法中常用。

#### 参数绑定和参数共享

在建立模型的初始，我们可能无法知道该使用什么样的参数，但可以根据相关领域和模型结构方面的知识得知模型参数之间应该存在一些相关性。比如我们正则化一个有监督模型的参数，使其接近另一个无监督模型的参数。构造的这种架构使得分类模型中的许多参数能与无监督模型中对
应的参数匹配。

参数范数惩罚是正则化参数使其彼此接近的一种方式，而更流行的方法是使用约束：强迫某些参数相等。由于我们将各种模型或模型组件解释为共享唯一的一组参数，这种正则化方法通常被称为参数共享（parameter sharing）。和正则化参数使其接近（通过范数惩罚）相比，参数共享的一个显著优点是，只有参数的子集需要被存储在内存中。对于某些特定模型，如卷积神经网络，这可能可以显著减少模型所占用的内存。

#### 稀疏表示

我们还有一种惩罚神经网络中的激活单元的策略，即稀疏化激活单元，这种策略间接地对模型参数施加了复杂惩罚。

#### Bagging和其他集成方法

Bagging策略是通过结合多个模型以降低泛化误差，常见的比如模型平均（model averaging），采用这种策略的技术称为集成方法。通俗的理解是三个臭皮匠赛过诸葛亮。

对于模型平均，假设我们有$$k$$个回归模型，每个模型的误差是$$\epsilon_i$$，且误差服从零均值方差为$$E[\epsilon^2_i]=v$$且协方差为$$E[\epsilon_i \epsilon_j]=c$$的多维正态分布，则集成模型的平均预测误差为$$\frac{1}{k}\sum_i \epsilon_i$$，平方误差的期望为：

$$E\left [(\frac{1}{k}\sum_i \epsilon_i)^2\right ]=\frac{1}{k^2}E\left [\sum_i (\epsilon^2_i+\sum_{i\neq j}\epsilon_i \epsilon_j)\right ]=\frac{1}{k}v+\frac{k-1}{k}c$$

可见，在误差完全相关时有$$c=v$$，均方误差为$$v$$，即模型平均没有用处；而在误差相关甚至完全相关$$c=0$$时，均方误差仅为$$\frac{1}{k}v$$，意味着集成方法带来了极大的且呈线性的提高。

现在流行的还有如Boosting、Blending、Stacking等集成方法。

#### Dropout

当模型的计算复杂度很高时，Bagging的代价就会很大，这时我们可以利用Dropout来实现一种廉价的Bagging近似，具体而言，Dropout训练的集成包括所有从基础网络除去非输出单元后形成的子网络，因而具有计算方便和易用性强的优点。

#### 对抗训练

神经网络已经在诸多领域取得了很好的成绩，但由于其过度线性化，微小的干扰也可能导致输出的巨大差异，如下图：
![adversarial](/img/post/2018_04_28_adversarial.png)
这时我们需要对抗训练来实现积极的正则化。
