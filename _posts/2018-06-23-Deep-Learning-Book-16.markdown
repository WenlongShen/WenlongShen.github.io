---
layout:     post
title:      "Deep Learning Book 学习笔记（16）"
subtitle:   "Structured Probabilistic Models for Deep Learning"
date:       2018-06-23
author:     "Wenlong Shen"
header-img: "img/bg/2018_3.jpg"
tags: 机器学习 读书笔记 2018
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

*有图有万物*

#### 非结构化建模的挑战

深度学习的目标是使得机器学习能够解决许多人工智能中亟需解决的挑战，这也意味着它们需要能够理解具有丰富结构的高维数据。大部分任务需要对输入数据整个结构的完整理解，所以并不能舍弃数据的一部分。这些任务包括以下几个：

* 估计密度函数：给定一个输入，机器学习系统返回一个对数据生成分布的真实密度函数的估计。这只需要一个输出，但它需要完全理解整个输入。即使向量中只有一个元素不太正常，系统也会给它赋予很低的概率。
* 去噪：给定一个受损的或者观察有误的输入数据，机器学习系统返回一个对原始的真实数据的估计。
* 缺失值的填补：给定输入的某些元素作为观察值，模型被要求返回一些或者全部未观察值的估计或者概率分布。
* 采样：模型从分布中抽取新的样本。其应用包括语音合成，即产生一个听起来很像人说话的声音。这个模型也需要多个输出以及对输入整体的良好建模。即使样本只有一个从错误分布中产生的元素，那么采样的过程也是错误的。

对上千甚至是上百万随机变量的分布建模，无论从计算上还是从统计意义上说，都是一个极具挑战性的任务。通常意义上讲，如果我们希望对一个包含$$n$$个离散变量并且每个变量都能取$$k$$个值的$$x$$的分布建模，那么最简单的表示$$P(x)$$的方法需要存储一个可以查询的表格。这个表格记录了每一种可能值的概率，则需要$$k^n$$个参数。基于下述几个原因，这种方式是不可行的：

* 内存：存储参数的开销。
* 统计的高效性。
* 运行时间：推断的开销。
* 运行时间：采样的开销。

结构化概率模型为随机变量之间的直接作用提供了一个正式的建模框架。这种方式大大减少了模型的参数个数以致于模型只需要更少的数据来进行有效的估计。这些更小的模型大大减小了在模型存储、模型推断以及从模型中采样时的计算开销。

#### 使用图描述模型结构

结构化概率模型使用图来表示随机变量之间的相互作用，每一个结点代表一个随机变量，每一条边代表一个直接相互作用。图模型可以被大致分为两类：基于有向无环图的模型和基于无向图的模型。

有向图模型（directed graphical model）是一种结构化概率模型，也被称为信念网络（belief network）或者贝叶斯网络（Bayesian network）。无向模型（undirected Model），也被称为马尔可夫随机场（Markov random field, MRF）或者是马尔可夫网络（Markov network）。

我们经常将特定的机器学习模型称为无向模型或有向模型。例如，我们通常将受限玻尔兹曼机称为无向模型，而稀疏编码则被称为有向模型。然而本质上并没有概率模型是有向或无向的，每个概率分布可以由有向模型或由无向模型表示。在最坏的情况下，我们可以使用“完全图”来表示任何分布。

#### 从图模型中采样

图模型简化了从模型中采样的过程。有向图模型的一个优点是，可以通过一个简单高效的过程从模型所表示的联合分布中产生样本，这个过程被称为原始采样，其非常快且非常简便。

原始采样的一个缺点是其仅适用于有向图模型，我们可以通过将无向模型转换为有向模型来实现从无向模型中抽样，但是这通常需要解决棘手的推断问题（要确定新有向图的根节点上的边缘分布），或者需要引入许多边从而会使得到的有向模型变得难以处理。

原始采样另一个缺点是它并不是每次采样都是条件采样操作。

#### 结构化建模的优势

使用结构化概率模型的主要优点是它们能够显著降低表示概率分布、学习和推断的成本。有向模型中采样还可以被加速，但是对于无向模型情况则较为复杂。选择不对某些变量的相互作用进行建模是允许所有这些操作使用较少的运行时间和内存的主要机制。图模型通过省略某些边来传达信息。在没有边的情况下，模型假设不对变量间直接的相互作用建模。

结构化概率模型允许我们明确地将给定的现有知识与知识的学习或者推断分开，这是一个不容易量化的益处。这使我们的模型更容易开发和调试。我们可以设计、分析和评估适用于更广范围的图的学习算法和推断算法。同时，我们可以设计能够捕捉到我们认为数据中存在的重要关系的模型。然后，我们可以组合这些不同的算法和结构，并获得不同可能性的笛卡尔乘积。然而，为每种可能的情况设计端到端的算法会更加困难。

#### 学习依赖关系

良好的生成模型需要准确地捕获所观察到的变量分布，通常这些不同元素彼此高度依赖。在深度学习中，最常用于建模这些依赖关系的方法是引入几个潜在或“隐藏”变量。如果一个良好的模型不包含任何潜变量，那么它在贝叶斯网络中的每个节点需要具有大量父节点或在马尔可夫网络中具有非常大的团。

当模型旨在描述直接连接的可见变量之间的依赖关系时，通常不可能连接所有变量，因此设计图模型时需要连接那些紧密相关的变量，并忽略其他变量之间的作用。

使用潜变量而不是自适应结构避免了离散搜索和多轮训练的需要。可见变量和潜变量之间的固定结构可以使用可见单元和隐藏单元之间的直接作用，从而建模可见单元之间的间接作用。使用简单的参数学习技术，我们可以学习到一个具有固定结构的模型，

#### 推断和近似推断

解决变量之间如何相互关联的问题是我们使用概率模型的一个主要方式。推断（inference）问题，即我们必须预测给定其他变量的情况下一些变量的值，或者在给定其他变量值的情况下预测一些变量的概率分布。

不幸的是，对于大多数有趣的深度模型来说，即使我们使用结构化图模型来简化这些推断问题，它们仍然是难以处理的。图结构允许我们用合理数量的参数来表示复杂的高维分布，但是用于深度学习的图并不满足这样的条件，从而难以实现高效地推断。

#### 结构化概率模型的深度学习方法

深度学习并不总是涉及特别深的图模型。在图模型中，我们可以根据图模型的图而不是计算图来定义模型的深度。

深度学习基本上总是利用分布式表示的思想。即使是用于深度学习目的的浅层模型（例如预训练浅层模型，稍后将形成深层模型），也几乎总是具有单个大的潜变量层。深度学习模型通常具有比可观察变量更多的潜变量。变量之间复杂的非线性相互作用通过多个潜变量的间接连接来实现。
